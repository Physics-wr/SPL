# encoding: utf-8
"""
@author:  liaoxingyu
@contact: liaoxingyu2@jd.com
"""

import copy
import itertools
from collections import defaultdict,OrderedDict
from typing import Optional, List
import logging
import random
import numpy as np
from torch.utils.data.sampler import Sampler
import time
import torch
from fastreid.utils import comm


def no_index(a, b):
    assert isinstance(a, list)
    return [i for i, j in enumerate(a) if j != b]


def reorder_index(batch_indices, world_size):
    r"""Reorder indices of samples to align with DataParallel training.
    In this order, each process will contain all images for one ID, triplet loss
    can be computed within each process, and BatchNorm will get a stable result.
    Args:
        batch_indices: A batched indices generated by sampler
        world_size: number of process
    Returns:

    """
    mini_batchsize = len(batch_indices) // world_size
    reorder_indices = []
    for i in range(0, mini_batchsize):
        for j in range(0, world_size):
            reorder_indices.append(batch_indices[i + j * mini_batchsize])
    return reorder_indices


class BalancedIdentitySampler(Sampler):
    def __init__(self, data_source: List, mini_batch_size: int, num_instances: int, seed: Optional[int] = None):
        self.data_source = data_source
        self.num_instances = num_instances
        self.num_pids_per_batch = mini_batch_size // self.num_instances

        self._rank = comm.get_rank()
        self._world_size = comm.get_world_size()
        self.batch_size = mini_batch_size * self._world_size

        self.index_pid = dict()
        self.pid_cam = defaultdict(list)
        self.pid_index = defaultdict(list)

        for index, info in enumerate(data_source):
            pid = info[1]
            camid = info[2]
            self.index_pid[index] = pid
            self.pid_cam[pid].append(camid)
            self.pid_index[pid].append(index)

        self.pids = sorted(list(self.pid_index.keys()))
        self.num_identities = len(self.pids)

        if seed is None:
            seed = comm.shared_random_seed()
        self._seed = int(seed)

        self._rank = comm.get_rank()
        self._world_size = comm.get_world_size()

    def __iter__(self):
        start = self._rank
        yield from itertools.islice(self._infinite_indices(), start, None, self._world_size)

    def _infinite_indices(self):
        np.random.seed(self._seed)
        while True:
            # Shuffle identity list
            identities = np.random.permutation(self.num_identities)

            # If remaining identities cannot be enough for a batch,
            # just drop the remaining parts
            drop_indices = self.num_identities % (self.num_pids_per_batch * self._world_size)
            if drop_indices: identities = identities[:-drop_indices]

            batch_indices = []
            for kid in identities:
                i = np.random.choice(self.pid_index[self.pids[kid]])
                _, i_pid, i_cam = self.data_source[i]
                batch_indices.append(i)
                pid_i = self.index_pid[i]
                cams = self.pid_cam[pid_i]
                index = self.pid_index[pid_i]
                select_cams = no_index(cams, i_cam)

                if select_cams:
                    if len(select_cams) >= self.num_instances:
                        cam_indexes = np.random.choice(select_cams, size=self.num_instances - 1, replace=False)
                    else:
                        cam_indexes = np.random.choice(select_cams, size=self.num_instances - 1, replace=True)
                    for kk in cam_indexes:
                        batch_indices.append(index[kk])
                else:
                    select_indexes = no_index(index, i)
                    if not select_indexes:
                        # Only one image for this identity
                        ind_indexes = [0] * (self.num_instances - 1)
                    elif len(select_indexes) >= self.num_instances:
                        ind_indexes = np.random.choice(select_indexes, size=self.num_instances - 1, replace=False)
                    else:
                        ind_indexes = np.random.choice(select_indexes, size=self.num_instances - 1, replace=True)

                    for kk in ind_indexes:
                        batch_indices.append(index[kk])

                if len(batch_indices) == self.batch_size:
                    yield from reorder_index(batch_indices, self._world_size)
                    batch_indices = []


class SetReWeightSampler(Sampler):
    def __init__(self, data_source: str, mini_batch_size: int, num_instances: int, set_weight: list,
                 seed: Optional[int] = None):
        self.data_source = data_source
        self.num_instances = num_instances
        self.num_pids_per_batch = mini_batch_size // self.num_instances

        self.set_weight = set_weight

        self._rank = comm.get_rank()
        self._world_size = comm.get_world_size()
        self.batch_size = mini_batch_size * self._world_size

        assert self.batch_size % (sum(self.set_weight) * self.num_instances) == 0 and \
               self.batch_size > sum(
            self.set_weight) * self.num_instances, "Batch size must be divisible by the sum set weight"

        self.index_pid = dict()
        self.pid_cam = defaultdict(list)
        self.pid_index = defaultdict(list)

        self.cam_pid = defaultdict(list)

        for index, info in enumerate(data_source):
            pid = info[1]
            camid = info[2]
            self.index_pid[index] = pid
            self.pid_cam[pid].append(camid)
            self.pid_index[pid].append(index)
            self.cam_pid[camid].append(pid)

        # Get sampler prob for each cam
        self.set_pid_prob = defaultdict(list)
        for camid, pid_list in self.cam_pid.items():
            index_per_pid = []
            for pid in pid_list:
                index_per_pid.append(len(self.pid_index[pid]))
            cam_image_number = sum(index_per_pid)
            prob = [i / cam_image_number for i in index_per_pid]
            self.set_pid_prob[camid] = prob

        self.pids = sorted(list(self.pid_index.keys()))
        self.num_identities = len(self.pids)

        if seed is None:
            seed = comm.shared_random_seed()
        self._seed = int(seed)

        self._rank = comm.get_rank()
        self._world_size = comm.get_world_size()

    def __iter__(self):
        start = self._rank
        yield from itertools.islice(self._infinite_indices(), start, None, self._world_size)

    def _infinite_indices(self):
        np.random.seed(self._seed)
        while True:
            batch_indices = []
            for camid in range(len(self.cam_pid.keys())):
                select_pids = np.random.choice(self.cam_pid[camid], size=self.set_weight[camid], replace=False,
                                               p=self.set_pid_prob[camid])
                for pid in select_pids:
                    index_list = self.pid_index[pid]
                    if len(index_list) > self.num_instances:
                        select_indexs = np.random.choice(index_list, size=self.num_instances, replace=False)
                    else:
                        select_indexs = np.random.choice(index_list, size=self.num_instances, replace=True)

                    batch_indices += select_indexs
            np.random.shuffle(batch_indices)

            if len(batch_indices) == self.batch_size:
                yield from reorder_index(batch_indices, self._world_size)


class NaiveIdentitySampler(Sampler):
    """
    Randomly sample N identities, then for each identity,
    randomly sample K instances, therefore batch size is N*K.
    Args:
    - data_source (list): list of (img_path, pid, camid).
    - num_instances (int): number of instances per identity in a batch.
    - batch_size (int): number of examples in a batch.
    """

    def __init__(self, data_source: str, mini_batch_size: int, num_instances: int, seed: Optional[int] = None):
        self.data_source = data_source
        self.num_instances = num_instances
        self.num_pids_per_batch = mini_batch_size // self.num_instances

        self._rank = comm.get_rank()
        self._world_size = comm.get_world_size()
        self.batch_size = mini_batch_size * self._world_size

        self.pid_index = defaultdict(list)

        for index, info in enumerate(data_source):
            pid = info[1]
            self.pid_index[pid].append(index)

        self.pids = sorted(list(self.pid_index.keys()))
        self.num_identities = len(self.pids)

        if seed is None:
            seed = comm.shared_random_seed()
        self._seed = int(seed)

    def __iter__(self):
        start = self._rank
        yield from itertools.islice(self._infinite_indices(), start, None, self._world_size)

    def _infinite_indices(self):
        np.random.seed(self._seed)
        while True:
            avl_pids = copy.deepcopy(self.pids)
            batch_idxs_dict = {}

            batch_indices = []
            while len(avl_pids) >= self.num_pids_per_batch:
                selected_pids = np.random.choice(avl_pids, self.num_pids_per_batch, replace=False).tolist()
                for pid in selected_pids:
                    # Register pid in batch_idxs_dict if not
                    if pid not in batch_idxs_dict:
                        idxs = copy.deepcopy(self.pid_index[pid])
                        if len(idxs) < self.num_instances:
                            idxs = np.random.choice(idxs, size=self.num_instances, replace=True).tolist()
                        np.random.shuffle(idxs)
                        batch_idxs_dict[pid] = idxs

                    avl_idxs = batch_idxs_dict[pid]
                    for _ in range(self.num_instances):
                        batch_indices.append(avl_idxs.pop(0))

                    if len(avl_idxs) < self.num_instances: avl_pids.remove(pid)

                if len(batch_indices) == self.batch_size:
                    yield from reorder_index(batch_indices, self._world_size)
                    batch_indices = []


class DomainIdentitySampler(Sampler):
    """
    Randomly sample N identities, THEY ALL BELONGS TO SAME TRAIN SET, then for each identity,
    randomly sample K instances, therefore batch size is N*K.
    Args:
    - data_source (list): list of (img_path, pid, camid).
    - num_instances (int): number of instances per identity in a batch.
    - batch_size (int): number of examples in a batch.
    """

    def __init__(self, data_source: str, mini_batch_size: int, num_instances: int, seed: Optional[int] = None):
        self.data_source = data_source
        self.num_instances = num_instances
        self.num_pids_per_batch = mini_batch_size // self.num_instances

        self._rank = comm.get_rank()
        self._world_size = comm.get_world_size()
        self.batch_size = mini_batch_size * self._world_size

        self.domain_pid_index= defaultdict(lambda:defaultdict(list))
        for index, info in enumerate(data_source):
            pid = info[1]
            domain=pid.split('_')[0]
            self.domain_pid_index[domain][pid].append(index)

        self.pids = {k:sorted(list(v.keys())) for k,v in self.domain_pid_index.items()}
        self.num_identities = 0
        for k in self.pids.keys():
            self.num_identities += len(self.pids[k])

        if seed is None:
            seed = comm.shared_random_seed()
        self._seed = int(seed)

    def __iter__(self):
        start = self._rank
        yield from itertools.islice(self._infinite_indices(), start, None, self._world_size)

    def _infinite_indices(self):
        np.random.seed(self._seed)
        keys=list(self.pids.keys())#str like "cuhksysu"
        domain_num=len(keys)
        avl_pids=defaultdict(list)
        batch_idxs_dict=defaultdict(dict)
        batch_indices=defaultdict(list)
        while True:
            i=np.random.randint(domain_num)# choose from domain i
            if(len(avl_pids[i])<self.num_pids_per_batch):
                avl_pids[i]=copy.deepcopy(self.pids[keys[i]])
                batch_idxs_dict[i].clear()
                batch_indices[i].clear()
            
            selected_pids = np.random.choice(avl_pids[i], self.num_pids_per_batch, replace=False).tolist()
            for pid in selected_pids:
                # Register pid in batch_idxs_dict if not
                if pid not in batch_idxs_dict[i]:
                    idxs = copy.deepcopy(self.domain_pid_index[keys[i]][pid])
                    if len(idxs) < self.num_instances:
                        idxs = np.random.choice(idxs, size=self.num_instances, replace=True).tolist()
                    np.random.shuffle(idxs)
                    batch_idxs_dict[i][pid] = idxs

                avl_idxs = batch_idxs_dict[i][pid]
                for _ in range(self.num_instances):
                    batch_indices[i].append(avl_idxs.pop(0))

                if len(avl_idxs) < self.num_instances: avl_pids[i].remove(pid)

            if len(batch_indices[i]) == self.batch_size:
                yield from reorder_index(batch_indices[i], self._world_size)
                batch_indices[i].clear()

def extract_batch_feature(model, inputs):
    with torch.no_grad():
        outputs = model(inputs)
    outputs = outputs.cpu()
    return outputs

def extract_features(model, data_loader, verbose=True, logger=None):
    fea_time = 0
    data_time = 0
    features = OrderedDict()
    labels = OrderedDict()
    end = time.time()

    if verbose and logger:
        logger.info('Feature extraction through model...')

    model = model.cuda().eval()
    with torch.no_grad():
        for i, batched_input in enumerate(data_loader):
            imgs=batched_input['images']
            pids=batched_input['targets']
            fnames=batched_input['img_paths']
            data_time += time.time() - end
            end = time.time()

            # outputs, f = extract_cnn_feature(model, imgs)
            outputs = extract_batch_feature(model, batched_input)
            for fname, output, pid in zip(fnames, outputs, pids):
                features[fname.split('/')[-1]] = output
                labels[fname.split('/')[-1]] = pid

            fea_time += time.time() - end
            end = time.time()
    model = model.train()

    if verbose and logger:
        logger.info('Feature time: {:.3f} seconds. Data time: {:.3f} seconds.'.format(fea_time, data_time))

    return features, labels

def euclidean_distance(qf, gf):
    m = qf.shape[0]
    n = gf.shape[0]
    dist_mat = torch.pow(qf, 2).sum(dim=1, keepdim=True).expand(m, n) + \
               torch.pow(gf, 2).sum(dim=1, keepdim=True).expand(n, m).t()
    dist_mat.addmm_(1, -2, qf, gf.t())
    return dist_mat

class SHS(Sampler):
    """
    Randomly sample N identities, THEY ALL BELONGS TO SAME TRAIN SET, then for each identity,
    randomly sample K instances, therefore batch size is N*K.
    Args:
    - data_source (list): list of (img_path, pid, camid).
    - num_instances (int): number of instances per identity in a batch.
    - batch_size (int): number of examples in a batch.
    """

    def __init__(self, data_source: str, mini_batch_size: int, num_instances: int, transforms,seed: Optional[int] = None):
        self.data_source = data_source
        self.num_instances = num_instances
        self.num_pids_per_batch = mini_batch_size // self.num_instances
        self.transform=transforms

        self._rank = comm.get_rank()
        self._world_size = comm.get_world_size()
        self.batch_size = mini_batch_size * self._world_size

        self.domain_pid_index= defaultdict(lambda:defaultdict(list))
        for index, info in enumerate(data_source):
            pid = info[1]
            domain=pid.split('_')[0]
            self.domain_pid_index[domain][pid].append(index)

        self.pids = {k:list(v.keys()) for k,v in self.domain_pid_index.items()}
        self.num_identities = 0
        for k in self.pids.keys():
            self.num_identities += len(self.pids[k])

        if seed is None:
            seed = comm.shared_random_seed()
        self._seed = int(seed)
        self.logger=logging.getLogger(__name__)

    def calc_distance(self,dataset,model):
        from torch.utils.data import DataLoader
        from fastreid.data.common import CommDataset
        data_loader = DataLoader(
            dataset=CommDataset(dataset,self.transform),
            batch_size=self.num_pids_per_batch, num_workers=8,
            shuffle=False, pin_memory=True)

        features, _ = extract_features(model, data_loader, logger=self.logger)
        features = torch.cat([features[fname.split('/')[-1]].unsqueeze(0) for fname, _, _ in dataset], 0)

        start = time.time()
        # dist_compute
        dist = euclidean_distance(features, features)

        return dist

    def refresh(self,model):
        self.logger.info('Style-aware Hard-negative Sampling.')
        domain_data_source={}
        for domain_name, domain_data in self.domain_pid_index.items():
            domain_data_source[domain_name]=[]
            for pid, index_list in domain_data.items():
                index = np.random.choice(index_list, size=1)[0]
                domain_data_source[domain_name].append(self.data_source[index])
        dist_mats={}
        for domain_name, domain_data in domain_data_source.items():
            dist_mat=self.calc_distance(domain_data,model)
            N = dist_mat.shape[0]
            mask = torch.eye(N,N, device=dist_mat.device) * 1e15
            dist_mat = dist_mat + mask
            dist_mats[domain_name]=dist_mat

        num_k = self.num_pids_per_batch- 1
        self.topk_index={}
        for domain_name,dist_mat in dist_mats.items():
            _, topk_index = torch.topk(dist_mat.cuda(), num_k, largest=False)
            self.topk_index[domain_name] = topk_index.cpu().numpy()


    def __iter__(self):
        start = self._rank
        yield from itertools.islice(self._infinite_indices(), start, None, self._world_size)

    def _infinite_indices(self):
        np.random.seed(self._seed)
        keys=list(self.pids.keys())#str like "cuhksysu"
        domain_num=len(keys)
        avl_pids=defaultdict(list)
        batch_idxs_dict=defaultdict(dict)
        batch_indices=defaultdict(list)
        while True:
            i=np.random.randint(domain_num)# choose from domain i
            if(len(avl_pids[i])<self.num_pids_per_batch):
                avl_pids[i]=copy.deepcopy(self.pids[keys[i]])
                batch_idxs_dict[i].clear()
                batch_indices[i].clear()
            
            if hasattr(self, 'topk_index'):
                anchor_pid = random.choice(avl_pids[i])
                ind = self.pids[keys[i]].index(anchor_pid)
                selected_pids = list(self.topk_index[keys[i]][ind])
                selected_pids = [self.pids[keys[i]][j] for j in selected_pids]
                selected_pids.append(anchor_pid)
                remove = 0
                avai_pids_rest = copy.deepcopy(avl_pids[i])
                selected_pids_cp = copy.deepcopy(selected_pids)
                for p in selected_pids_cp:
                    if p not in avl_pids[i]:
                        selected_pids.remove(p)
                        remove += 1
                    else:
                        avai_pids_rest.remove(p)
                add_pids = random.sample(avai_pids_rest, remove)
                del(avai_pids_rest)
                del(selected_pids_cp)
                selected_pids.extend(add_pids)
            else:
                selected_pids = np.random.choice(avl_pids[i], self.num_pids_per_batch, replace=False).tolist()
            for pid in selected_pids:
                # Register pid in batch_idxs_dict if not
                if pid not in batch_idxs_dict[i]:
                    idxs = copy.deepcopy(self.domain_pid_index[keys[i]][pid])
                    if len(idxs) < self.num_instances:
                        idxs = np.random.choice(idxs, size=self.num_instances, replace=True).tolist()
                    np.random.shuffle(idxs)
                    batch_idxs_dict[i][pid] = idxs

                avl_idxs = batch_idxs_dict[i][pid]
                for _ in range(self.num_instances):
                    batch_indices[i].append(avl_idxs.pop(0))

                if len(avl_idxs) < self.num_instances: avl_pids[i].remove(pid)

            if len(batch_indices[i]) == self.batch_size:
                yield from reorder_index(batch_indices[i], self._world_size)
                batch_indices[i].clear()



        # while True:
        #     avl_pids = copy.deepcopy(self.pids[keys[i]])
        #     batch_idxs_dict = {}

        #     batch_indices = []
        #     while len(avl_pids) >= self.num_pids_per_batch:
        #         selected_pids = np.random.choice(avl_pids, self.num_pids_per_batch, replace=False).tolist()
        #         for pid in selected_pids:
        #             # Register pid in batch_idxs_dict if not
        #             if pid not in batch_idxs_dict:
        #                 idxs = copy.deepcopy(self.domain_pid_index[keys[i]][pid])
        #                 if len(idxs) < self.num_instances:
        #                     idxs = np.random.choice(idxs, size=self.num_instances, replace=True).tolist()
        #                 np.random.shuffle(idxs)
        #                 batch_idxs_dict[pid] = idxs

        #             avl_idxs = batch_idxs_dict[pid]
        #             for _ in range(self.num_instances):
        #                 batch_indices.append(avl_idxs.pop(0))

        #             if len(avl_idxs) < self.num_instances: avl_pids.remove(pid)

        #         if len(batch_indices) == self.batch_size:
        #             yield from reorder_index(batch_indices, self._world_size)
        #             batch_indices = []

        #     i+=1
        #     if i==domain_num:
        #         i=0
# import logging
# import time
# import random
# class DomainIdentitySampler(Sampler):
#     """
#     all ids are from one domain in a batch.
#     """

#     def __init__(self, data_source, batch_size, num_instances, num_pids):
#         self.data_source = data_source
#         self.batch_size = batch_size
#         self.num_instances = num_instances
#         self.num_pids_per_batch = self.batch_size // self.num_instances
#         self.index_dic = defaultdict(list) #dict with list value
#         #{783: [0, 5, 116, 876, 1554, 2041],...,}
#         for index, (_, pid, _) in enumerate(self.data_source):
#             self.index_dic[pid].append(index)
#         self.pids = list(self.index_dic.keys())
#         self.num_pids = num_pids

#         # estimate number of examples in an epoch
#         self.length = 0
#         for pid in self.pids:
#             idxs = self.index_dic[pid]
#             num = len(idxs)
#             if num < self.num_instances:
#                 num = self.num_instances
#             self.length += num - num % self.num_instances

#     def __iter__(self):
#         logger = logging.getLogger(__name__)
#         logger.info("All ids from the same domain in a batch. Start batch dividing.")
#         t0 = time.time()
#         batch_idxs_dict = defaultdict(list)
#         for pid in self.pids:
#             idxs = copy.deepcopy(self.index_dic[pid])
#             if len(idxs) < self.num_instances:
#                 idxs = np.random.choice(idxs, size=self.num_instances, replace=True)
#             random.shuffle(idxs)
#             batch_idxs = []
#             for idx in idxs:
#                 batch_idxs.append(idx)
#                 if len(batch_idxs) == self.num_instances:
#                     batch_idxs_dict[pid].append(batch_idxs)
#                     batch_idxs = []

#         avai_pids = copy.deepcopy(self.pids)
#         final_idxs = []

#         num_dom = len(self.num_pids)
#         start, end = [0], [self.num_pids[0]]
#         for i in range(num_dom-1):
#             start.append(start[-1] + self.num_pids[i])
#             end.append(start[-1] + self.num_pids[i+1])
#         pids_domain_wise = [
#             avai_pids[start[i]:end[i]]\
#             for i in range(len(self.num_pids))
#         ]
#         remain_pids = []
        
#         while len(pids_domain_wise) > 0:
#             pids = random.choice(pids_domain_wise)
#             ind = pids_domain_wise.index(pids)
#             if len(pids) < self.num_pids_per_batch:
#                 remain_pids.extend(pids)
#                 pids_domain_wise.remove(pids)
#                 if len(remain_pids)>=self.num_pids_per_batch:
#                     selected_pids = random.sample(remain_pids, self.num_pids_per_batch)
#                 else:
#                     continue
#             else:
#                 selected_pids = random.sample(pids, self.num_pids_per_batch)
#             for pid in selected_pids:
#                 batch_idxs = batch_idxs_dict[pid].pop(0)
#                 final_idxs.extend(batch_idxs)
#                 if len(batch_idxs_dict[pid]) == 0:
#                     if pid in remain_pids:
#                         remain_pids.remove(pid)
#                         continue
#                     pids_domain_wise[ind].remove(pid)
#                     if len(pids_domain_wise[ind]) == 0:
#                         pids_domain_wise.remove(pids)

#         logger.info('batch divide time: {:.2f}s'.format(time.time()-t0))
#         return iter(final_idxs)
    
#     def __len__(self):
#         return self.length